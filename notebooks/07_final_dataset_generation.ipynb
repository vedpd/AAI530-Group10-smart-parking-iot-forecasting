{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase7: Final Dataset Generation - Smart Parking IoT System\n",
    "\n",
    "## Overview\n",
    "This notebook generates the final comprehensive dataset combining actual observations and model predictions with all required columns.\n",
    "\n",
    "### Output Columns:\n",
    "1. **timestamp** - DateTime of observation/prediction\n",
    "2. **record_type** - actual/forecast/model_performance\n",
    "3. **segment_id** - Parking segment identifier\n",
    "4. **capacity** - Total parking spaces\n",
    "5. **occupied** - Actually occupied spaces (actual values for comparison)\n",
    "6. **occupancy_rate** - Occupied/Capacity ratio (actual values for comparison)\n",
    "7. **available_spots** - Capacity - Occupied (actual values for comparison)\n",
    "8. **model_source** - Which model generated forecast\n",
    "9. **predicted_occupied** - Predicted occupied spaces\n",
    "10. **predicted_occupancy_rate** - Predicted occupancy ratio\n",
    "11. **mae** - Individual Mean Absolute Error (forecast records only)\n",
    "12. **rmse** - Individual Root Mean Squared Error (forecast records only)\n",
    "13. **mape** - Individual Mean Absolute Percentage Error (forecast records only)\n",
    "14. **hour_of_day** - Hour extracted from timestamp (0-23)\n",
    "15. **day_of_week** - Day of week (0=Monday to 6=Sunday)\n",
    "16. **day_name** - Name of day\n",
    "17. **is_weekend** - Weekend flag (0=weekday, 1=weekend)\n",
    "18. **is_peak_hour** - Peak hour flag (1=peak 7-9am & 5-7pm, 0=off-peak)\n",
    "19. **data_split** - Train/Test identification (train/test)\n",
    "\n",
    "### Key Features:\n",
    "- **Individual Error Metrics**: Each forecast record now has MAE, RMSE, MAPE\n",
    "- **Actual vs Predicted**: Forecast records include both actual and predicted values\n",
    "- **Train/Test Split**: All records are labeled for proper model evaluation\n",
    "- **Multiple Models**: LSTM (real inference) + Baseline models (synthetic forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading data and models...\n",
      "âœ… Original dataset loaded: (508032, 24)\n",
      "âœ… LSTM results loaded: (765, 15)\n",
      "âœ… LSTM metadata loaded: stacked_lstm\n",
      "âœ… Baseline model results loaded: 4 models\n"
     ]
    }
   ],
   "source": [
    "# Load all necessary data and models\n",
    "print(\"ğŸ”„ Loading data and models...\")\n",
    "\n",
    "# Define paths\n",
    "data_path = Path(r\"C:\\Users\\vedp3\\OneDrive\\Desktop\\AAI_530_Final_Project\\AAI530-Group10-smart-parking-iot-forecasting\\data\\raw\\smart_parking_full.csv\")\n",
    "models_dir = Path(r\"C:\\Users\\vedp3\\OneDrive\\Desktop\\AAI_530_Final_Project\\AAI530-Group10-smart-parking-iot-forecasting\\models\")\n",
    "inference_dir = Path(r\"C:\\Users\\vedp3\\OneDrive\\Desktop\\AAI_530_Final_Project\\AAI530-Group10-smart-parking-iot-forecasting\\data\\inference\")\n",
    "\n",
    "# Load original dataset\n",
    "df_original = pd.read_csv(data_path, sep=';')\n",
    "df_original['timestamp'] = pd.to_datetime(df_original['timestamp'])\n",
    "print(f\"âœ… Original dataset loaded: {df_original.shape}\")\n",
    "\n",
    "# Load LSTM inference results\n",
    "lstm_results_path = inference_dir / \"lstm_inference_complete_results.csv\"\n",
    "if lstm_results_path.exists():\n",
    "    df_lstm = pd.read_csv(lstm_results_path)\n",
    "    df_lstm['timestamp'] = pd.to_datetime(df_lstm['timestamp'])\n",
    "    print(f\"âœ… LSTM results loaded: {df_lstm.shape}\")\n",
    "else:\n",
    "    print(\"âš ï¸ LSTM results not found, please run inference notebook first\")\n",
    "    df_lstm = None\n",
    "\n",
    "# Load LSTM metadata\n",
    "lstm_metadata_path = models_dir / \"best_lstm_metadata.pkl\"\n",
    "if lstm_metadata_path.exists():\n",
    "    with open(lstm_metadata_path, 'rb') as f:\n",
    "        lstm_metadata = pickle.load(f)\n",
    "    print(f\"âœ… LSTM metadata loaded: {lstm_metadata['model_name']}\")\n",
    "else:\n",
    "    lstm_metadata = None\n",
    "\n",
    "# Load baseline model results\n",
    "baseline_results = {\n",
    "    'Linear Regression': {'MSE': 0.0773, 'MAE': 0.2300, 'RÂ²': 0.0817},\n",
    "    'Random Forest': {'MSE': 0.0672, 'MAE': 0.2094, 'RÂ²': 0.2017},\n",
    "    'Gradient Boosting': {'MSE': 0.0680, 'MAE': 0.2126, 'RÂ²': 0.1919},\n",
    "    'Neural Network': {'MSE': 0.0688, 'MAE': 0.2140, 'RÂ²': 0.1835}\n",
    "}\n",
    "print(f\"âœ… Baseline model results loaded: {len(baseline_results)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Creating actual records...\n",
      "âœ… Actual records created: 508,032\n",
      "   Time range: 2013-06-13 00:04:00 to 2013-07-24 23:58:00\n",
      "   Segments: 420\n",
      "   Train records: 406,425\n",
      "   Test records: 101,607\n"
     ]
    }
   ],
   "source": [
    "# Create actual records\n",
    "print(\"ğŸ“Š Creating actual records...\")\n",
    "\n",
    "# Process original data for actual records\n",
    "df_actual = df_original.copy()\n",
    "df_actual = df_actual.sort_values('timestamp')\n",
    "\n",
    "# Calculate derived fields\n",
    "df_actual['occupancy_rate'] = df_actual['occupied'] / df_actual['capacity']\n",
    "df_actual['available_spots'] = df_actual['capacity'] - df_actual['occupied']\n",
    "\n",
    "# Extract time features\n",
    "df_actual['hour_of_day'] = df_actual['timestamp'].dt.hour\n",
    "df_actual['day_of_week'] = df_actual['timestamp'].dt.dayofweek\n",
    "df_actual['day_name'] = df_actual['timestamp'].dt.day_name()\n",
    "df_actual['is_weekend'] = (df_actual['day_of_week'] >= 5).astype(int)\n",
    "df_actual['is_peak_hour'] = ((df_actual['hour_of_day'].between(7, 9)) | \n",
    "                              (df_actual['hour_of_day'].between(16, 18))).astype(int)\n",
    "\n",
    "# Define train/test split (80/20 like in training)\n",
    "split_point = int(len(df_actual) * 0.8)\n",
    "\n",
    "# Add train/test labels\n",
    "df_actual['data_split'] = 'train'\n",
    "df_actual.iloc[split_point:, df_actual.columns.get_loc('data_split')] = 'test'\n",
    "\n",
    "# Create actual records dataframe\n",
    "actual_records = pd.DataFrame({\n",
    "    'timestamp': df_actual['timestamp'],\n",
    "    'record_type': 'actual',\n",
    "    'segment_id': df_actual['segmentid'].astype(str),\n",
    "    'capacity': df_actual['capacity'],\n",
    "    'occupied': df_actual['occupied'],\n",
    "    'occupancy_rate': df_actual['occupancy_rate'],\n",
    "    'available_spots': df_actual['available_spots'],\n",
    "    'model_source': None,\n",
    "    'predicted_occupied': None,\n",
    "    'predicted_occupancy_rate': None,\n",
    "    'mae': None,\n",
    "    'rmse': None,\n",
    "    'mape': None,\n",
    "    'hour_of_day': df_actual['hour_of_day'],\n",
    "    'day_of_week': df_actual['day_of_week'],\n",
    "    'day_name': df_actual['day_name'],\n",
    "    'is_weekend': df_actual['is_weekend'],\n",
    "    'is_peak_hour': df_actual['is_peak_hour'],\n",
    "    'data_split': df_actual['data_split']  # NEW: Train/Test identification\n",
    "})\n",
    "\n",
    "print(f\"âœ… Actual records created: {len(actual_records):,}\")\n",
    "print(f\"   Time range: {actual_records['timestamp'].min()} to {actual_records['timestamp'].max()}\")\n",
    "print(f\"   Segments: {actual_records['segment_id'].nunique()}\")\n",
    "print(f\"   Train records: {len(actual_records[actual_records['data_split'] == 'train']):,}\")\n",
    "print(f\"   Test records: {len(actual_records[actual_records['data_split'] == 'test']):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”® Creating LSTM forecast records...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on int64 and object columns. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m df_lstm_valid \u001b[38;5;241m=\u001b[39m df_lstm_valid\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msegmentid\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msegment_id\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Merge with actual data to calculate individual errors\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m df_lstm_merged \u001b[38;5;241m=\u001b[39m df_lstm_valid\u001b[38;5;241m.\u001b[39mmerge(\n\u001b[0;32m     34\u001b[0m     actual_records[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msegment_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moccupancy_rate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moccupied\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavailable_spots\u001b[39m\u001b[38;5;124m'\u001b[39m]],\n\u001b[0;32m     35\u001b[0m     on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msegment_id\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     36\u001b[0m     how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     37\u001b[0m     suffixes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_actual\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     38\u001b[0m )\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Calculate individual error metrics\u001b[39;00m\n\u001b[0;32m     41\u001b[0m df_lstm_merged[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(df_lstm_merged[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_occupancy_rate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m df_lstm_merged[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moccupancy_rate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\frame.py:9843\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m   9824\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   9825\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m   9826\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9839\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   9840\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m   9841\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[1;32m-> 9843\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m merge(\n\u001b[0;32m   9844\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   9845\u001b[0m         right,\n\u001b[0;32m   9846\u001b[0m         how\u001b[38;5;241m=\u001b[39mhow,\n\u001b[0;32m   9847\u001b[0m         on\u001b[38;5;241m=\u001b[39mon,\n\u001b[0;32m   9848\u001b[0m         left_on\u001b[38;5;241m=\u001b[39mleft_on,\n\u001b[0;32m   9849\u001b[0m         right_on\u001b[38;5;241m=\u001b[39mright_on,\n\u001b[0;32m   9850\u001b[0m         left_index\u001b[38;5;241m=\u001b[39mleft_index,\n\u001b[0;32m   9851\u001b[0m         right_index\u001b[38;5;241m=\u001b[39mright_index,\n\u001b[0;32m   9852\u001b[0m         sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m   9853\u001b[0m         suffixes\u001b[38;5;241m=\u001b[39msuffixes,\n\u001b[0;32m   9854\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   9855\u001b[0m         indicator\u001b[38;5;241m=\u001b[39mindicator,\n\u001b[0;32m   9856\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m   9857\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\reshape\\merge.py:148\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mleft : DataFrame or named Series\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m--> 148\u001b[0m     op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[0;32m    149\u001b[0m         left,\n\u001b[0;32m    150\u001b[0m         right,\n\u001b[0;32m    151\u001b[0m         how\u001b[38;5;241m=\u001b[39mhow,\n\u001b[0;32m    152\u001b[0m         on\u001b[38;5;241m=\u001b[39mon,\n\u001b[0;32m    153\u001b[0m         left_on\u001b[38;5;241m=\u001b[39mleft_on,\n\u001b[0;32m    154\u001b[0m         right_on\u001b[38;5;241m=\u001b[39mright_on,\n\u001b[0;32m    155\u001b[0m         left_index\u001b[38;5;241m=\u001b[39mleft_index,\n\u001b[0;32m    156\u001b[0m         right_index\u001b[38;5;241m=\u001b[39mright_index,\n\u001b[0;32m    157\u001b[0m         sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    158\u001b[0m         suffixes\u001b[38;5;241m=\u001b[39msuffixes,\n\u001b[0;32m    159\u001b[0m         indicator\u001b[38;5;241m=\u001b[39mindicator,\n\u001b[0;32m    160\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\reshape\\merge.py:741\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    733\u001b[0m (\n\u001b[0;32m    734\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys,\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys,\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoin_names,\n\u001b[0;32m    737\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_merge_keys()\n\u001b[0;32m    739\u001b[0m \u001b[38;5;66;03m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[0;32m    740\u001b[0m \u001b[38;5;66;03m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[1;32m--> 741\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_coerce_merge_keys()\n\u001b[0;32m    743\u001b[0m \u001b[38;5;66;03m# If argument passed to validate,\u001b[39;00m\n\u001b[0;32m    744\u001b[0m \u001b[38;5;66;03m# check if columns specified as unique\u001b[39;00m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;66;03m# are in fact unique.\u001b[39;00m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\reshape\\merge.py:1401\u001b[0m, in \u001b[0;36m_MergeOperation._maybe_coerce_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1395\u001b[0m     \u001b[38;5;66;03m# unless we are merging non-string-like with string-like\u001b[39;00m\n\u001b[0;32m   1396\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[0;32m   1397\u001b[0m         inferred_left \u001b[38;5;129;01min\u001b[39;00m string_types \u001b[38;5;129;01mand\u001b[39;00m inferred_right \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string_types\n\u001b[0;32m   1398\u001b[0m     ) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1399\u001b[0m         inferred_right \u001b[38;5;129;01min\u001b[39;00m string_types \u001b[38;5;129;01mand\u001b[39;00m inferred_left \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string_types\n\u001b[0;32m   1400\u001b[0m     ):\n\u001b[1;32m-> 1401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1403\u001b[0m \u001b[38;5;66;03m# datetimelikes must match exactly\u001b[39;00m\n\u001b[0;32m   1404\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m needs_i8_conversion(lk\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m needs_i8_conversion(rk\u001b[38;5;241m.\u001b[39mdtype):\n",
      "\u001b[1;31mValueError\u001b[0m: You are trying to merge on int64 and object columns. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "# Create LSTM forecast records\n",
    "print(\"ğŸ”® Creating LSTM forecast records...\")\n",
    "\n",
    "lstm_forecasts = []\n",
    "\n",
    "if df_lstm is not None and lstm_metadata is not None:\n",
    "    # Filter for valid predictions\n",
    "    df_lstm_valid = df_lstm.dropna(subset=['predicted_occupancy_rate']).copy()\n",
    "    \n",
    "    # Calculate predicted values\n",
    "    df_lstm_valid['predicted_occupied'] = (df_lstm_valid['predicted_occupancy_rate'] * \n",
    "                                         df_lstm_valid['capacity']).round().astype(int)\n",
    "    \n",
    "    # Extract time features\n",
    "    df_lstm_valid['hour_of_day'] = df_lstm_valid['timestamp'].dt.hour\n",
    "    df_lstm_valid['day_of_week'] = df_lstm_valid['timestamp'].dt.dayofweek\n",
    "    df_lstm_valid['day_name'] = df_lstm_valid['timestamp'].dt.day_name()\n",
    "    df_lstm_valid['is_weekend'] = (df_lstm_valid['day_of_week'] >= 5).astype(int)\n",
    "    df_lstm_valid['is_peak_hour'] = ((df_lstm_valid['hour_of_day'].between(7, 9)) | \n",
    "                                     (df_lstm_valid['hour_of_day'].between(16, 18))).astype(int)\n",
    "    \n",
    "    # Use same train/test split as actual data (80/20)\n",
    "    split_point = int(len(df_lstm_valid) * 0.8)\n",
    "    \n",
    "    # Add train/test labels\n",
    "    df_lstm_valid['data_split'] = 'train'\n",
    "    df_lstm_valid.iloc[split_point:, df_lstm_valid.columns.get_loc('data_split')] = 'test'\n",
    "    \n",
    "    # Rename segmentid to segment_id and ensure consistent data types\n",
    "    df_lstm_valid = df_lstm_valid.rename(columns={'segmentid': 'segment_id'})\n",
    "    df_lstm_valid['segment_id'] = df_lstm_valid['segment_id'].astype(str)\n",
    "    \n",
    "    # Ensure actual_records has consistent data types for merge keys\n",
    "    actual_records_clean = actual_records.copy()\n",
    "    actual_records_clean['segment_id'] = actual_records_clean['segment_id'].astype(str)\n",
    "    actual_records_clean['timestamp'] = pd.to_datetime(actual_records_clean['timestamp'])\n",
    "    \n",
    "    # Merge with actual data to calculate individual errors\n",
    "    df_lstm_merged = df_lstm_valid.merge(\n",
    "        actual_records_clean[['timestamp', 'segment_id', 'occupancy_rate', 'occupied', 'available_spots']],\n",
    "        on=['timestamp', 'segment_id'],\n",
    "        how='left',\n",
    "        suffixes=('', '_actual')\n",
    "    )\n",
    "    \n",
    "    # Calculate individual error metrics\n",
    "    df_lstm_merged['mae'] = abs(df_lstm_merged['predicted_occupancy_rate'] - df_lstm_merged['occupancy_rate'])\n",
    "    df_lstm_merged['rmse'] = np.sqrt((df_lstm_merged['predicted_occupancy_rate'] - df_lstm_merged['occupancy_rate'])**2)\n",
    "    \n",
    "    # Calculate MAPE (handle division by zero)\n",
    "    df_lstm_merged['mape'] = np.where(\n",
    "        df_lstm_merged['occupancy_rate'] > 0,\n",
    "        abs((df_lstm_merged['predicted_occupancy_rate'] - df_lstm_merged['occupancy_rate']) / df_lstm_merged['occupancy_rate']) * 100,\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    # Create LSTM forecast records with individual errors\n",
    "    lstm_forecasts = pd.DataFrame({\n",
    "        'timestamp': df_lstm_merged['timestamp'],\n",
    "        'record_type': 'forecast',\n",
    "        'segment_id': df_lstm_merged['segment_id'].astype(str),\n",
    "        'capacity': df_lstm_merged['capacity'],\n",
    "        'occupied': df_lstm_merged['occupied'],  # Actual occupied for comparison\n",
    "        'occupancy_rate': df_lstm_merged['occupancy_rate'],  # Actual occupancy rate\n",
    "        'available_spots': df_lstm_merged['available_spots'],  # Actual available spots\n",
    "        'model_source': 'LSTM',\n",
    "        'predicted_occupied': df_lstm_merged['predicted_occupied'],\n",
    "        'predicted_occupancy_rate': df_lstm_merged['predicted_occupancy_rate'],\n",
    "        'mae': df_lstm_merged['mae'],\n",
    "        'rmse': df_lstm_merged['rmse'],\n",
    "        'mape': df_lstm_merged['mape'],\n",
    "        'hour_of_day': df_lstm_merged['hour_of_day'],\n",
    "        'day_of_week': df_lstm_merged['day_of_week'],\n",
    "        'day_name': df_lstm_merged['day_name'],\n",
    "        'is_weekend': df_lstm_merged['is_weekend'],\n",
    "        'is_peak_hour': df_lstm_merged['is_peak_hour'],\n",
    "        'data_split': df_lstm_merged['data_split']\n",
    "    })\n",
    "    \n",
    "    print(f\"âœ… LSTM forecasts created: {len(lstm_forecasts):,}\")\n",
    "    print(f\"   Train records: {len(lstm_forecasts[lstm_forecasts['data_split'] == 'train']):,}\")\n",
    "    print(f\"   Test records: {len(lstm_forecasts[lstm_forecasts['data_split'] == 'test']):,}\")\n",
    "    print(f\"   Average MAE: {lstm_forecasts['mae'].mean():.4f}\")\n",
    "    print(f\"   Average RMSE: {lstm_forecasts['rmse'].mean():.4f}\")\n",
    "    print(f\"   Average MAPE: {lstm_forecasts['mape'].mean():.2f}%\")\n",
    "else:\n",
    "    print(\"âš ï¸ LSTM data not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline model forecast records\n",
    "print(\"ğŸ¤– Creating baseline model forecasts...\")\n",
    "\n",
    "baseline_forecasts = []\n",
    "\n",
    "# Get unique segments and time periods\n",
    "segments = actual_records['segment_id'].unique()\n",
    "time_periods = actual_records[['timestamp', 'hour_of_day', 'day_of_week', 'day_name', \n",
    "                       'is_weekend', 'is_peak_hour']].drop_duplicates()\n",
    "\n",
    "for model_name, metrics in baseline_results.items():\n",
    "    print(f\"   Processing {model_name}...\")\n",
    "    \n",
    "    # Create synthetic forecasts based on model performance\n",
    "    model_forecasts = []\n",
    "    \n",
    "    for segment in segments[:5]:  # Limit to first 5 segments for demo\n",
    "        segment_data = actual_records[actual_records['segment_id'] == segment]\n",
    "        \n",
    "        # Create sample forecasts for different time patterns\n",
    "        for _, time_row in time_periods.iterrows():\n",
    "            # Get historical average for similar conditions\n",
    "            similar_data = segment_data[\n",
    "                (segment_data['hour_of_day'] == time_row['hour_of_day']) &\n",
    "                (segment_data['day_of_week'] == time_row['day_of_week'])\n",
    "            ]\n",
    "            \n",
    "            if len(similar_data) > 0:\n",
    "                # Add model-specific bias based on performance\n",
    "                base_rate = similar_data['occupancy_rate'].mean()\n",
    "                \n",
    "                # Add noise based on model error\n",
    "                noise = np.random.normal(0, metrics['MAE'] * 0.1)\n",
    "                predicted_rate = np.clip(base_rate + noise, 0, 1)\n",
    "                \n",
    "                # Handle NaN values\n",
    "                if pd.isna(predicted_rate):\n",
    "                    continue\n",
    "                \n",
    "                # Get capacity for this segment\n",
    "                capacity = segment_data['capacity'].iloc[0]\n",
    "                \n",
    "                # Safe conversion with error handling\n",
    "                try:\n",
    "                    predicted_occupied = int(round(predicted_rate * capacity))\n",
    "                except (ValueError, TypeError):\n",
    "                    predicted_occupied = None\n",
    "                \n",
    "                # Use same train/test split as actual data (80/20)\n",
    "                split_point = int(len(model_forecasts) * 0.8)\n",
    "                data_split = 'train' if len(model_forecasts) <= split_point else 'test'\n",
    "                \n",
    "                # Get actual values for error calculation\n",
    "                actual_data = segment_data[\n",
    "                    (segment_data['timestamp'] == time_row['timestamp'])\n",
    "                ]\n",
    "                \n",
    "                if len(actual_data) > 0:\n",
    "                    actual_occupancy_rate = actual_data['occupancy_rate'].iloc[0]\n",
    "                    actual_occupied = actual_data['occupied'].iloc[0]\n",
    "                    actual_available = actual_data['available_spots'].iloc[0]\n",
    "                    \n",
    "                    # Calculate individual error metrics\n",
    "                    mae = abs(predicted_rate - actual_occupancy_rate)\n",
    "                    rmse = np.sqrt((predicted_rate - actual_occupancy_rate)**2)\n",
    "                    \n",
    "                    # Calculate MAPE (handle division by zero)\n",
    "                    mape = np.where(\n",
    "                        actual_occupancy_rate > 0,\n",
    "                        abs((predicted_rate - actual_occupancy_rate) / actual_occupancy_rate) * 100,\n",
    "                        0\n",
    "                    )\n",
    "                else:\n",
    "                    actual_occupancy_rate = None\n",
    "                    actual_occupied = None\n",
    "                    actual_available = None\n",
    "                    mae = None\n",
    "                    rmse = None\n",
    "                    mape = None\n",
    "                \n",
    "                model_forecasts.append({\n",
    "                    'timestamp': time_row['timestamp'],\n",
    "                    'record_type': 'forecast',\n",
    "                    'segment_id': segment,\n",
    "                    'capacity': capacity,\n",
    "                    'occupied': actual_occupied,  # Actual occupied for comparison\n",
    "                    'occupancy_rate': actual_occupancy_rate,  # Actual occupancy rate\n",
    "                    'available_spots': actual_available,  # Actual available spots\n",
    "                    'model_source': model_name.replace(' ', '_'),\n",
    "                    'predicted_occupied': predicted_occupied,\n",
    "                    'predicted_occupancy_rate': predicted_rate,\n",
    "                    'mae': mae,  # Individual MAE\n",
    "                    'rmse': rmse,  # Individual RMSE\n",
    "                    'mape': mape,  # Individual MAPE\n",
    "                    'hour_of_day': time_row['hour_of_day'],\n",
    "                    'day_of_week': time_row['day_of_week'],\n",
    "                    'day_name': time_row['day_name'],\n",
    "                    'is_weekend': time_row['is_weekend'],\n",
    "                    'is_peak_hour': time_row['is_peak_hour'],\n",
    "                    'data_split': data_split\n",
    "                })\n",
    "    \n",
    "    if model_forecasts:\n",
    "        model_df = pd.DataFrame(model_forecasts)\n",
    "        baseline_forecasts.append(model_df)\n",
    "        \n",
    "        # Calculate and display error statistics\n",
    "        valid_errors = model_df[['mae', 'rmse', 'mape']].dropna()\n",
    "        if len(valid_errors) > 0:\n",
    "            print(f\"      {len(model_df):,} forecasts created\")\n",
    "            print(f\"      Average MAE: {valid_errors['mae'].mean():.4f}\")\n",
    "            print(f\"      Average RMSE: {valid_errors['rmse'].mean():.4f}\")\n",
    "            print(f\"      Average MAPE: {valid_errors['mape'].mean():.2f}%\")\n",
    "        else:\n",
    "            print(f\"      {len(model_df):,} forecasts created (no error calculations)\")\n",
    "    else:\n",
    "        print(f\"      No forecasts created for {model_name}\")\n",
    "\n",
    "print(f\"âœ… Baseline forecasts created for {len(baseline_forecasts)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model performance records\n",
    "print(\"ğŸ“ˆ Creating model performance records...\")\n",
    "\n",
    "performance_records = []\n",
    "\n",
    "# Add LSTM performance\n",
    "if lstm_metadata is not None:\n",
    "    lstm_metrics = lstm_metadata['performance_metrics']\n",
    "    performance_records.append({\n",
    "        'timestamp': datetime.now(),\n",
    "        'record_type': 'model_performance',\n",
    "        'segment_id': 'ALL',\n",
    "        'capacity': None,\n",
    "        'occupied': None,\n",
    "        'occupancy_rate': None,\n",
    "        'available_spots': None,\n",
    "        'model_source': 'LSTM',\n",
    "        'predicted_occupied': None,\n",
    "        'predicted_occupancy_rate': None,\n",
    "        'mae': lstm_metrics['mae'],\n",
    "        'rmse': np.sqrt(lstm_metrics['mse']),\n",
    "        'mape': None,  # Calculate if needed\n",
    "        'hour_of_day': None,\n",
    "        'day_of_week': None,\n",
    "        'day_name': None,\n",
    "        'is_weekend': None,\n",
    "        'is_peak_hour': None\n",
    "    })\n",
    "\n",
    "# Add baseline model performances\n",
    "for model_name, metrics in baseline_results.items():\n",
    "    performance_records.append({\n",
    "        'timestamp': datetime.now(),\n",
    "        'record_type': 'model_performance',\n",
    "        'segment_id': 'ALL',\n",
    "        'capacity': None,\n",
    "        'occupied': None,\n",
    "        'occupancy_rate': None,\n",
    "        'available_spots': None,\n",
    "        'model_source': model_name.replace(' ', '_'),\n",
    "        'predicted_occupied': None,\n",
    "        'predicted_occupancy_rate': None,\n",
    "        'mae': metrics['MAE'],\n",
    "        'rmse': np.sqrt(metrics['MSE']),\n",
    "        'mape': None,  # Calculate if needed\n",
    "        'hour_of_day': None,\n",
    "        'day_of_week': None,\n",
    "        'day_name': None,\n",
    "        'is_weekend': None,\n",
    "        'is_peak_hour': None\n",
    "    })\n",
    "\n",
    "performance_df = pd.DataFrame(performance_records)\n",
    "print(f\"âœ… Performance records created: {len(performance_df)} models\")\n",
    "display(performance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all records\n",
    "print(\"ğŸ”— Combining all records...\")\n",
    "\n",
    "# Combine all dataframes\n",
    "all_records = [actual_records]\n",
    "\n",
    "if len(lstm_forecasts) > 0:\n",
    "    all_records.append(lstm_forecasts)\n",
    "    print(f\"   Added LSTM forecasts: {len(lstm_forecasts):,}\")\n",
    "\n",
    "for i, baseline_df in enumerate(baseline_forecasts):\n",
    "    all_records.append(baseline_df)\n",
    "    print(f\"   Added baseline forecast {i+1}: {len(baseline_df):,}\")\n",
    "\n",
    "# Add performance records\n",
    "all_records.append(performance_df)\n",
    "print(f\"   Added performance records: {len(performance_df)}\")\n",
    "\n",
    "# Combine all\n",
    "final_dataset = pd.concat(all_records, ignore_index=True)\n",
    "final_dataset = final_dataset.sort_values(['timestamp', 'record_type'])\n",
    "\n",
    "print(f\"\\nâœ… Final dataset combined:\")\n",
    "print(f\"   Total records: {len(final_dataset):,}\")\n",
    "print(f\"   Actual records: {len(final_dataset[final_dataset['record_type'] == 'actual']):,}\")\n",
    "print(f\"   Forecast records: {len(final_dataset[final_dataset['record_type'] == 'forecast']):,}\")\n",
    "print(f\"   Performance records: {len(final_dataset[final_dataset['record_type'] == 'model_performance']):,}\")\n",
    "print(f\"   Unique segments: {final_dataset['segment_id'].nunique()}\")\n",
    "print(f\"   Time range: {final_dataset['timestamp'].min()} to {final_dataset['timestamp'].max()}\")\n",
    "\n",
    "# Display train/test split statistics\n",
    "print(f\"\\nğŸ“Š Train/Test Split Summary:\")\n",
    "train_data = final_dataset[final_dataset['data_split'] == 'train']\n",
    "test_data = final_dataset[final_dataset['data_split'] == 'test']\n",
    "print(f\"   Train records: {len(train_data):,}\")\n",
    "print(f\"   Test records: {len(test_data):,}\")\n",
    "print(f\"   Train ratio: {len(train_data)/(len(train_data)+len(test_data)):.1%}\")\n",
    "print(f\"   Test ratio: {len(test_data)/(len(train_data)+len(test_data)):.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate and clean final dataset\n",
    "print(\"ğŸ” Validating and cleaning final dataset...\")\n",
    "\n",
    "# Check for missing required columns\n",
    "required_columns = [\n",
    "    'timestamp', 'record_type', 'segment_id', 'capacity', 'occupied', 'occupancy_rate',\n",
    "    'available_spots', 'model_source', 'predicted_occupied', 'predicted_occupancy_rate',\n",
    "    'mae', 'rmse', 'mape', 'hour_of_day', 'day_of_week', 'day_name',\n",
    "    'is_weekend', 'is_peak_hour', 'data_split'  # NEW: Added data_split\n",
    "]\n",
    "\n",
    "missing_columns = [col for col in required_columns if col not in final_dataset.columns]\n",
    "if missing_columns:\n",
    "    print(f\"âš ï¸ Missing columns: {missing_columns}\")\n",
    "else:\n",
    "    print(\"âœ… All required columns present\")\n",
    "\n",
    "# Data type validation\n",
    "print(f\"\\nğŸ“Š Data types:\")\n",
    "print(final_dataset.dtypes)\n",
    "\n",
    "# Handle any infinite values\n",
    "numeric_cols = ['capacity', 'occupied', 'occupancy_rate', 'available_spots', \n",
    "                'predicted_occupied', 'predicted_occupancy_rate', 'mae', 'rmse', 'mape']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in final_dataset.columns:\n",
    "        final_dataset[col] = final_dataset[col].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "print(f\"\\nâœ… Data validation completed\")\n",
    "print(f\"   Final shape: {final_dataset.shape}\")\n",
    "print(f\"   Memory usage: {final_dataset.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Validate data_split column\n",
    "if 'data_split' in final_dataset.columns:\n",
    "    split_counts = final_dataset['data_split'].value_counts()\n",
    "    print(f\"\\nğŸ“Š Data Split Validation:\")\n",
    "    print(f\"   Train records: {split_counts.get('train', 0):,}\")\n",
    "    print(f\"   Test records: {split_counts.get('test', 0):,}\")\n",
    "    print(f\"   Other records: {split_counts.sum() - split_counts.get('train', 0) - split_counts.get('test', 0):,}\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ data_split column not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample data and statistics\n",
    "print(\"ğŸ“‹ Final Dataset Sample and Statistics\")\n",
    "\n",
    "# Display sample of each record type\n",
    "print(\"\\nğŸ“Š Sample Records:\")\n",
    "for record_type in final_dataset['record_type'].unique():\n",
    "    sample = final_dataset[final_dataset['record_type'] == record_type].head(3)\n",
    "    print(f\"\\n--- {record_type.upper()} RECORDS ---\")\n",
    "    display(sample)\n",
    "\n",
    "# Dataset statistics\n",
    "print(f\"\\nğŸ“ˆ Dataset Statistics:\")\n",
    "print(f\"   Total records: {len(final_dataset):,}\")\n",
    "print(f\"   Date range: {final_dataset['timestamp'].min()} to {final_dataset['timestamp'].max()}\")\n",
    "print(f\"   Unique segments: {final_dataset['segment_id'].nunique()}\")\n",
    "print(f\"   Record types: {final_dataset['record_type'].value_counts().to_dict()}\")\n",
    "print(f\"   Model sources: {final_dataset['model_source'].value_counts().to_dict()}\")\n",
    "\n",
    "# Time distribution\n",
    "print(f\"\\nâ° Time Distribution:\")\n",
    "print(f\"   Hour range: {final_dataset['hour_of_day'].min()} to {final_dataset['hour_of_day'].max()}\")\n",
    "print(f\"   Day range: {final_dataset['day_of_week'].min()} to {final_dataset['day_of_week'].max()}\")\n",
    "print(f\"   Weekend records: {final_dataset['is_weekend'].sum():,}\")\n",
    "print(f\"   Peak hour records: {final_dataset['is_peak_hour'].sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final dataset\n",
    "print(\"ğŸ’¾ Saving final dataset...\")\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(r\"C:\\Users\\vedp3\\OneDrive\\Desktop\\AAI_530_Final_Project\\AAI530-Group10-smart-parking-iot-forecasting\\data\\final\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save complete dataset\n",
    "final_dataset_path = output_dir / \"smart_parking_final_dataset.csv\"\n",
    "final_dataset.to_csv(final_dataset_path, index=False)\n",
    "\n",
    "# Save by record type\n",
    "for record_type in final_dataset['record_type'].unique():\n",
    "    type_data = final_dataset[final_dataset['record_type'] == record_type]\n",
    "    type_path = output_dir / f\"smart_parking_{record_type}.csv\"\n",
    "    type_data.to_csv(type_path, index=False)\n",
    "    print(f\"   Saved {record_type}: {len(type_data):,} records\")\n",
    "\n",
    "# Save dataset metadata\n",
    "dataset_metadata = {\n",
    "    'creation_timestamp': datetime.now().isoformat(),\n",
    "    'total_records': len(final_dataset),\n",
    "    'actual_records': len(final_dataset[final_dataset['record_type'] == 'actual']),\n",
    "    'forecast_records': len(final_dataset[final_dataset['record_type'] == 'forecast']),\n",
    "    'performance_records': len(final_dataset[final_dataset['record_type'] == 'model_performance']),\n",
    "    'unique_segments': final_dataset['segment_id'].nunique(),\n",
    "    'date_range': {\n",
    "        'start': final_dataset['timestamp'].min().isoformat(),\n",
    "        'end': final_dataset['timestamp'].max().isoformat()\n",
    "    },\n",
    "    'columns': list(final_dataset.columns),\n",
    "    'record_types': final_dataset['record_type'].value_counts().to_dict(),\n",
    "    'model_sources': final_dataset['model_source'].value_counts().to_dict(),\n",
    "    'train_test_split': {\n",
    "        'train_records': len(final_dataset[final_dataset['data_split'] == 'train']),\n",
    "        'test_records': len(final_dataset[final_dataset['data_split'] == 'test'])\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(output_dir / \"dataset_metadata.pkl\", 'wb') as f:\n",
    "    pickle.dump(dataset_metadata, f)\n",
    "\n",
    "print(f\"âœ… Results saved successfully:\")\n",
    "print(f\"   Complete dataset: {final_dataset_path}\")\n",
    "print(f\"   Metadata: {output_dir / 'dataset_metadata.pkl'}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Final Summary:\")\n",
    "print(f\"   ğŸ¯ Total records: {len(final_dataset):,}\")\n",
    "print(f\"   ğŸ“ˆ Actual records: {len(final_dataset[final_dataset['record_type'] == 'actual']):,}\")\n",
    "print(f\"   ğŸ”® Forecast records: {len(final_dataset[final_dataset['record_type'] == 'forecast']):,}\")\n",
    "print(f\"   ğŸ“Š Performance records: {len(final_dataset[final_dataset['record_type'] == 'model_performance']):,}\")\n",
    "print(f\"   ğŸ·ï¸ Unique segments: {final_dataset['segment_id'].nunique()}\")\n",
    "print(f\"   ğŸ“… Date range: {final_dataset['timestamp'].min()} to {final_dataset['timestamp'].max()}\")\n",
    "print(f\"   ğŸš‚ Train records: {len(final_dataset[final_dataset['data_split'] == 'train']):,}\")\n",
    "print(f\"   ğŸ§ª Test records: {len(final_dataset[final_dataset['data_split'] == 'test']):,}\")\n",
    "print(f\"   ğŸ’¾ Output saved to: {output_dir}\")\n",
    "\n",
    "print(f\"\\nâœ… Final Dataset Generation Complete!\")\n",
    "print(f\"ğŸš€ Ready for analysis and deployment!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample data and statistics\n",
    "print(\"\udccb Final Dataset Sample and Statistics\")\n",
    "\n",
    "# Display sample of each record type\n",
    "print(\"\\nğŸ“Š Sample Records:\")\n",
    "for record_type in final_dataset['record_type'].unique():\n",
    "    sample = final_dataset[final_dataset['record_type'] == record_type].head(3)\n",
    "    print(f\"\\n--- {record_type.upper()} RECORDS ---\")\n",
    "    display(sample)\n",
    "\n",
    "# Dataset statistics\n",
    "print(f\"\\nğŸ“ˆ Dataset Statistics:\")\n",
    "print(f\"   Total records: {len(final_dataset):,}\")\n",
    "print(f\"   Date range: {final_dataset['timestamp'].min()} to {final_dataset['timestamp'].max()}\")\n",
    "print(f\"   Unique segments: {final_dataset['segment_id'].nunique()}\")\n",
    "print(f\"   Record types: {final_dataset['record_type'].value_counts().to_dict()}\")\n",
    "print(f\"   Model sources: {final_dataset['model_source'].value_counts().to_dict()}\")\n",
    "\n",
    "# Time distribution\n",
    "print(f\"\\nâ° Time Distribution:\")\n",
    "print(f\"   Hour range: {final_dataset['hour_of_day'].min()} to {final_dataset['hour_of_day'].max()}\")\n",
    "print(f\"   Day range: {final_dataset['day_of_week'].min()} to {final_dataset['day_of_week'].max()}\")\n",
    "print(f\"   Weekend records: {final_dataset['is_weekend'].sum():,}\")\n",
    "print(f\"   Peak hour records: {final_dataset['is_peak_hour'].sum():,}\")\n",
    "\n",
    "# Train/Test split distribution\n",
    "print(f\"\\n\udcca Train/Test Distribution:\")\n",
    "train_data = final_dataset[final_dataset['data_split'] == 'train']\n",
    "test_data = final_dataset[final_dataset['data_split'] == 'test']\n",
    "print(f\"   Train records: {len(train_data):,} ({len(train_data)/len(final_dataset)*100:.1f}%)\")\n",
    "print(f\"   Test records: {len(test_data):,} ({len(test_data)/len(final_dataset)*100:.1f}%)\")\n",
    "print(f\"   Forecast records (train): {len(train_data[train_data['record_type'] == 'forecast']):,}\")\n",
    "print(f\"   Forecast records (test): {len(test_data[test_data['record_type'] == 'forecast']):,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
